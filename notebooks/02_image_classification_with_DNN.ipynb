{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Dataset and split into partitions\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Create a validation set from training images\n",
    "validation_images = train_images[:5000]\n",
    "validation_labels = train_labels[:5000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Analyze the stuctural properties of the training dataset\n",
    "print(\"Shape of training images\", train_images.shape)\n",
    "print(\"Shape of training labels\", train_labels.shape)\n",
    "print(\"Number of training labels\",  train_labels[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Alternatively you can use the Python len() method\n",
    "print(\"Number of training images\", len(train_images))\n",
    "print(\"Number of training labels\", len(train_labels))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Analyze the stuctural properties of the test dataset\n",
    "print(\"Shape of images\", test_images.shape)\n",
    "print(\"Shape of labels\", test_labels.shape)\n",
    "print(\"Number of labels\",  test_labels[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Alternatively you can use the Python len() method\n",
    "print(\"Number of test images\", len(test_images))\n",
    "print(\"Number of test labels\", len(test_labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Example of training label content\n",
    "print(train_labels[0])\n",
    "print(\"Minimum value of training labels\", train_labels.min())\n",
    "print(\"Maximum value of training labels\", train_labels.max())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Content of training images before normalisation\n",
    "print(train_images[0])\n",
    "print(\"Minimum value of training images\", train_images.min())\n",
    "print(\"Maximum value of training images\", train_images.max())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: Test that this works on TensorFlow version 2.1+\n",
    "# Normalizing trianing images through keras preprocessing layers\n",
    "normalization_layer = preprocessing.Normalization()\n",
    "normalization_layer.adapt(train_images)\n",
    "train_images = normalization_layer(train_images)\n",
    "\n",
    "# Normalize test images\n",
    "normalization_layer.adapt(test_images)\n",
    "test_images = normalization_layer(test_images)\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalise training and test images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "validation_images = validation_images / 255.0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Content of training images after normalisation\n",
    "print(\"Minimum value of training images\", train_images.min())\n",
    "print(\"Maximum value of training images\", train_images.max())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Label\tDescription\n",
    "# 0\tT-shirt/top\n",
    "# 1\tTrouser\n",
    "# 2\tPullover\n",
    "# 3\tDress\n",
    "# 4\tCoat\n",
    "# 5\tSandal\n",
    "# 6\tShirt\n",
    "# 7\tSneaker\n",
    "# 8\tBag\n",
    "# 9\tAnkle boot\n",
    "fashion_mnist_class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "fashion_mnist_class_names[train_labels[2]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising data\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(fashion_mnist_class_names[train_labels[i]])\n",
    "    plt.imshow(train_images[i])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classification MLP(Multilayer perceptron) with three hidden layers\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(500, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(250, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(100, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(10, activation=keras.activations.softmax)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model compilation and initialisation of optimizer\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Configure TensorBoard storage location\n",
    "root_logdir = os.path.join(os.curdir, \"runs\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "# Train model\n",
    "model.fit(train_images, train_labels, epochs=60, validation_data=(validation_images, validation_labels), callbacks=[tensorboard_cb, early_stopping_cb])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_evaluation_results = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"The test loss is\", model_evaluation_results[0])\n",
    "print(\"The test accuracy is\", model_evaluation_results[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prediction on test images using model.predict() method\n",
    "practical_test_images =  test_images[:10]\n",
    "prediction_probabilites = model.predict(practical_test_images)\n",
    "prediction_probabilites"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Altertive 1: Clean up model prediction using argmax to find the largest probablity\n",
    "def derive_predicted_classes(prediction_probabilites):\n",
    "    batch_prediction = []\n",
    "    for vector in prediction_probabilites:\n",
    "        batch_prediction.append(np.argmax(vector))\n",
    "    return batch_prediction\n",
    "    \n",
    "model_prediction = derive_predicted_classes(prediction_probabilites)\n",
    "model_prediction\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Altertive 2: Get specific model preiction using the model.predict_classes() method\n",
    "model_prediction = model.predict_classes(practical_test_images)\n",
    "model_prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.array(fashion_mnist_class_names)[model_prediction]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualise the prediction result\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(len(practical_test_images)):\n",
    "    plt.subplot(5,5, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(False)\n",
    "    plt.imshow(practical_test_images[i])\n",
    "    plt.title(fashion_mnist_class_names[model_prediction[i]])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"image_classification_model.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: If you encounter the error \"AttributeError: 'str' object has no attribute 'decode'\" when trying to load the model\n",
    "# this is caused by using an higher version of h5py. Simply downgrade the package version of h5py.\n",
    "# using pip \"pip install 'h5py==2.10.0' --force-reinstall\"\n",
    "# or using conda \"conda install -c anaconda h5py=2.10.0\"\n",
    "#Â reference: https://stackoverflow.com/questions/53740577/does-any-one-got-attributeerror-str-object-has-no-attribute-decode-whi\n",
    "\n",
    "loaded_model = keras.models.load_model(\"image_classification_model.h5\")\n",
    "predictions = loaded_model.predict_classes(practical_test_images)\n",
    "print(predictions)\n",
    "print(np.array(fashion_mnist_class_names)[predictions])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run Tensorboard using the command below from the location of the run folder\n",
    "# tensorboard --logdir='runs'"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11  ('deepcourse': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "dc00d3c18918e3d7b202c635280a2defd5a3767f6fcbbc54f3a1ab7d5490b6ae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}