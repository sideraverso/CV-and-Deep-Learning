{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing a Deep Neural Network\n",
    "\n",
    "This notebook details the steps taken to implement a Deep Neural Network(DNN). To build our neural network, the libraies TensorFlow and Keras are leveraged in order to provide the features and methods required to build the components of a neural network.\n",
    "\n",
    "TensorFlow: An open-source platform for the implementation, training, and deployment of machine learning models.\n",
    "Keras: An open-source library used for the implementation of neural network architectures that run on both CPUs and GPUs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "\n",
    "Installing Tensorflow and Keras is simple when using a package managers such as pip or conda.\n",
    "\n",
    "Here are examples of installing TensorFlow.\n",
    "And it's worth mentioning that you don’t need to explicitly install keras as it’s a built in API within TensorFlow 2+\n",
    "\n",
    "If you have a CPU you can install TensorFlow with the following command\n",
    "“conda install tensorflow”\n",
    "\n",
    "If you have a GPU installed, you can install TensorFlow with the following command\n",
    "“conda install tensorflow-gpu”"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**You can verify a successful installation of TensorFlow in several ways**\n",
    "\n",
    "1. Observing the TensorFlow package in your environment by running the command\n",
    "“conda list”\n",
    "\n",
    "2. Second is by importing TensorFlow into your notebook and checking the version programmatically\n",
    "tf.version.VERSION\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Verifying installation of Tensorflow\n",
    "tf.version.VERSION"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned earlier Keras provides tools required to implement the classification model. \n",
    "Keras houses the Model API, which provides several methods, components and classes required to implement neural networks\n",
    "\n",
    "The three various ways you can create keras Models are through the: Sequential model, Functional API and Model subclassing.\n",
    "Today we will only be exploring the Sequential model method of implementing neural networks.\n",
    "\n",
    "1. **The Sequential model**, which is very straightforward (a list of layers), but is limited to single-input, single-output stacks of layers (as the name gives away).\n",
    "2. **The Functional API**, which is an easy-to-use, fully-featured API that supports arbitrary model architectures. For most people and most use cases, this is what you should be using. This is the Keras \"industry strength\" model.\n",
    "3. **Model Subclassing**, where you implement everything from scratch on your own. Use this if you have complex, out-of-the-box research use cases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Implementation Components\n",
    "\n",
    "1. Keras Layers API (keras.layers)\n",
    "Layers within Keras allow for the composition of neural networks as they are the fundemental components of neural networks in Keras.\n",
    "Layers within Keras also house the following: weights of the neural network and functions that act upon inputs and provide outputs, typically to the next layer \n",
    "\n",
    "2. Flatten Layer (keras.layers.Flatten)\n",
    "The Flatten class acts upon the inputs by reducing the dimensionality of the input data to one.\n",
    "Image datasets are multidimensional, and in order for input data to be feed forward input data through the neural network the dimensions of the input data needs to be reduced to one, essentially we require our input data to be a 1-dimensional.\n",
    "For example, an input to the Flatten layer with the shape (None, 10, 2) will provide the output (None, 20).\n",
    "\n",
    "The input shape of the first layer of a neural network should match the shape of the input data, hence the 'input_shape' attribute of the Flatten layer is (28,28) when using the FashionMNIST dataset (shown in the notebook 02_image_classification_with_DNN).\n",
    "\n",
    "3. Dense Layer (keras.layers.Dense)\n",
    "The dense layer houses neurons within the neural network. the number of neurons within a dense layer is specified by the 'unit' attribute. All neurons/units within the dense layer receives input from the previous layer.\n",
    "The dense layer operation on its input is a matrix vector muliplication between the input data, learnable weights of the layer and biases.\n",
    "\n",
    "4. Activation Functions (keras.activations.relu / keras.activations.softmax)\n",
    "Activation Function: A mathematical operation that transforms the result or signals of neurons into a normalized output. An activation function is a component of a neural network that introduces non-linearity within the network. The inclusion of the activation function enables the neural network to have greater representational power and solve complex functions.\n",
    "\n",
    "**Examples of Activation functions**\n",
    "ReLU activation: Stands for ‘rectified linear unit’ ( y=max(0, x)). It’s a type of activation function that transforms the value results of a neuron. The transformation imposed by ReLU on values from a neuron is represented by the formula y=max(0,x). The ReLU activation function clamps down any negative values from the neuron to 0, and positive values remain unchanged. The result of this mathematical transformation is utilized as the output of the current layer, and as input to the next.\n",
    "\n",
    "Softmax: An activation function that is utilized to derive the probability distribution of a set of numbers within an input vector. The output of a softmax activation function is a vector in which its set of values represents the probability of an occurrence of a class/event. The values within the vector all add up to 1.\n",
    "\n",
    "\n",
    "**Below is a deep neural network, containing three hidden layer, an input layer and one output layer and it's built using the Keras Sequential API.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(500, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(250, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(100, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(10, activation=keras.activations.softmax)\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-20 20:57:00.696056: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-20 20:57:00.697741: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Structural Information\n",
    "\n",
    "A structural summary of the neural network implemented above is obtainable by calling the ‘summary’ method available on our model. By calling the summary method, we gain information on the model properties such as layers, layer type, shapes, number of weights in the model, and layers.\n",
    "\n",
    "[Keras documentation reference](https://keras.io/api/models/model/#summary-method)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 543,860\n",
      "Trainable params: 543,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conducting analysis on a neural network internal components is useful in machine learning, especially when you are not the creator of the neural network.\n",
    "Keras has provided several methods of getting the interal details of an already built neural network.\n",
    "\n",
    "1. Using the python's list subscript functionality.\n",
    "2. Using the Keras's 'get_layers' method.\n",
    "\n",
    "How to implement and use both methods of getting neural network information are shown below. In the next code snippets we are analysing the weights and baises of the second layer (first hidden layer) of the neural network implemented above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "first_hidden_layer = model.layers[1]\n",
    "weights, biases = first_hidden_layer.weights\n",
    "print(weights)\n",
    "print('\\n')\n",
    "print(biases)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'dense/kernel:0' shape=(784, 500) dtype=float32, numpy=\n",
      "array([[ 0.05093002,  0.05504215,  0.05177955, ...,  0.02047996,\n",
      "        -0.0625891 , -0.00962639],\n",
      "       [ 0.06531806, -0.0597941 ,  0.04932003, ..., -0.01232168,\n",
      "        -0.06414726,  0.01078124],\n",
      "       [-0.01519692, -0.01787981, -0.04564763, ..., -0.0055357 ,\n",
      "        -0.04718521,  0.01353937],\n",
      "       ...,\n",
      "       [-0.05444261,  0.01340163,  0.03171267, ...,  0.01427417,\n",
      "        -0.04084084,  0.02765889],\n",
      "       [ 0.02220616, -0.01719176,  0.02869142, ...,  0.02767232,\n",
      "        -0.03098669, -0.01445924],\n",
      "       [ 0.03595876, -0.03129721,  0.03073858, ..., -0.05191932,\n",
      "        -0.00325928, -0.04973669]], dtype=float32)>\n",
      "\n",
      "\n",
      "<tf.Variable 'dense/bias:0' shape=(500,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "layer = model.get_layer(index=1)\n",
    "print(layer.weights)\n",
    "print('\\n')\n",
    "print(layer.bias)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(784, 500) dtype=float32, numpy=\n",
      "array([[ 0.05093002,  0.05504215,  0.05177955, ...,  0.02047996,\n",
      "        -0.0625891 , -0.00962639],\n",
      "       [ 0.06531806, -0.0597941 ,  0.04932003, ..., -0.01232168,\n",
      "        -0.06414726,  0.01078124],\n",
      "       [-0.01519692, -0.01787981, -0.04564763, ..., -0.0055357 ,\n",
      "        -0.04718521,  0.01353937],\n",
      "       ...,\n",
      "       [-0.05444261,  0.01340163,  0.03171267, ...,  0.01427417,\n",
      "        -0.04084084,  0.02765889],\n",
      "       [ 0.02220616, -0.01719176,  0.02869142, ...,  0.02767232,\n",
      "        -0.03098669, -0.01445924],\n",
      "       [ 0.03595876, -0.03129721,  0.03073858, ..., -0.05191932,\n",
      "        -0.00325928, -0.04973669]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(500,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n",
      "\n",
      "\n",
      "<tf.Variable 'dense/bias:0' shape=(500,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('deepcourse': conda)"
  },
  "interpreter": {
   "hash": "dc00d3c18918e3d7b202c635280a2defd5a3767f6fcbbc54f3a1ab7d5490b6ae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}