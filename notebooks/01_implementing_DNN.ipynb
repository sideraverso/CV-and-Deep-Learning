{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing a Deep Neural Network\n",
    "\n",
    "This notebook details the steps taken to implement a Deep Neural Network(DNN). To build our neural network, the libraries TensorFlow and Keras are leveraged to provide the features and methods required to build the components of a neural network.\n",
    "\n",
    "**[TensorFlow](https://www.tensorflow.org/)**: An open-source platform for implementing, training, and deploying machine learning models.\n",
    "\n",
    "**[Keras](https://keras.io/)**: An open-source library used to implement neural network architectures that run on both CPUs and GPUs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "\n",
    "Installing Tensorflow and Keras is straightforward when using package managers such as [pip](https://www.tensorflow.org/install) or [conda](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/).\n",
    "\n",
    "Here are examples of installing TensorFlow.\n",
    "And it's worth mentioning that you don't need to explicitly install Keras as it's a built-in API within TensorFlow 2+\n",
    "\n",
    "If you have a CPU, you can install TensorFlow with the following command\n",
    "`conda install tensorflow`\n",
    "\n",
    "If you have a GPU installed, you can install TensorFlow with the following command\n",
    "`conda install tensorflow-gpu`\n",
    "\n",
    "Alternatively, if you are using pip\n",
    "`pip install tensorflow`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**You can verify a successful installation of TensorFlow in several ways**\n",
    "\n",
    "1. Observing the TensorFlow package in your environment by running the command\n",
    "`conda list` on the teriminal\n",
    "\n",
    "2. Second is by importing TensorFlow into your notebook and checking the version programmatically\n",
    "tf.version.VERSION\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Verifying installation of Tensorflow\n",
    "tf.version.VERSION"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Keras library provides tools required to implement the image classification model. \n",
    "Keras houses the Model API, which provides several methods, components and classes required to implement neural networks.\n",
    "\n",
    "The three various ways you can create [keras Models](https://keras.io/api/models/) are through the: Sequential model, Functional API and Model subclassing.\n",
    "Today we will only be exploring the Sequential model method of implementing neural networks.\n",
    "\n",
    "**The Sequential Model** allows for the implementation of a neural network through the use of consecutive layers. The layers within a sequential model accept single input and produce a single output result.\n",
    "\n",
    "**The Functional Model** This method of implementing a neural network allows for a robust number of features and more engineering flexibility.\n",
    "\n",
    "**Model Subclassing** Full engineering flexibility is enabled with this method of implementing neural networks. Novel, arbitrary and complex neural network components are built from scratch the subclassing method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Implementation Components\n",
    "\n",
    "1. [ Keras Layers API](https://keras.io/api/layers) (keras.layers)\n",
    "\n",
    "    Layers within Keras allow for the composition of neural networks as they are the fundamental components of neural networks in Keras.\n",
    "    Layers within Keras also house the following: weights of the neural network and functions that act upon inputs and provide outputs, typically to the next layer. \n",
    "\n",
    "2. [Flatten Layer](https://keras.io/api/layers/reshaping_layers/flatten/) (keras.layers.Flatten)\n",
    "\n",
    "    The  Flatten layer is known as one of the reshaping layers Keras provides to modify the dimensionalities of inputs.\n",
    "    The Flatten class acts upon the inputs by reducing the dimensionality of the input data to one.\n",
    "    Image datasets are multidimensional, and for input data to be fed forward through the neural network, the dimensions of the input data need to be reduced to one. We essentially require our input data to be 1-dimensional.\n",
    "    For example, an input to the Flatten layer with the shape (None, 10, 2) will provide the output (None, 20).\n",
    "\n",
    "    The input shape of the first layer of a neural network should match the shape of the input data. Hence the 'input_shape' attribute of the Flatten layer is (28,28) when using the FashionMNIST dataset (shown in the notebook 02_image_classification_with_DNN).\n",
    "\n",
    "3. [Dense Layer](https://keras.io/api/layers/core_layers/dense/) (keras.layers.Dense)\n",
    "\n",
    "    The dense layer houses neurons within the neural network. The 'unit' attribute specifies the number of neurons within a dense layer. All neurons/units within the dense layer receive input from the previous layer.\n",
    "    The dense layer operation on its input is a matrix-vector multiplication between the input data, learnable weights of the layer, plus the biases.\n",
    "\n",
    "4. [Activation Functions](https://keras.io/api/layers/activations/) (keras.activations.relu / keras.activations.softmax)\n",
    "\n",
    "    Activation Function: A mathematical operation that transforms the result or signals of neurons into a normalized output. An activation function is a component of a neural network that introduces non-linearity within the network. The inclusion of the activation function enables the neural network to have greater representational power and solve complex functions.\n",
    "\n",
    "**Examples of Activation functions**\n",
    "\n",
    "- ReLU activation: Stands for ‘rectified linear unit’ ( y=max(0, x)). It's a type of activation function that transforms the value results of a neuron. The transformation imposed by ReLU on values from a neuron is represented by the formula y=max(0,x). The ReLU activation function clamps down any negative values from the neuron to 0, and positive values remain unchanged. The result of this mathematical transformation is utilized as the output of the current layer, and as input to the next.\n",
    "\n",
    "- Softmax: An activation function utilized to derive the probability distribution of a set of numbers within an input vector. The output of a softmax activation function is a vector whose set of values represents the probability of an occurrence of a class/event. The values within the vector all add up to 1.\n",
    "\n",
    "\n",
    "**Below is a deep neural network containing three hidden layers, an input layer and one output layer and built using the Keras Sequential API.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(500, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(250, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(100, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(10, activation=keras.activations.softmax)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Structural Information\n",
    "\n",
    "A structural summary of the neural network implemented above is obtainable by calling our model's 'summary' method. By calling the summary method, we gain information on the model properties such as layers, layer type, shapes, number of weights in the model, and layers.\n",
    "\n",
    "[Keras documentation reference](https://keras.io/api/models/model/#summary-method)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 543,860\n",
      "Trainable params: 543,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysing a neural network internal components is useful in machine learning, especially when you are not the creator of the neural network.\n",
    "Keras has provided several methods of getting the internal details of an already built neural network.\n",
    "\n",
    "1. Using python's list subscript functionality.\n",
    "2. Using the Keras's ['get_layers'](https://keras.io/api/models/model/#getlayer-method) method.\n",
    "\n",
    "How to implement and use both methods of getting neural network information are presented in the cells below. In the following code snippets, we analyse the weights and biases of the second layer (which is also first hidden layer) of the neural network implemented above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "first_hidden_layer = model.layers[1]\n",
    "weights, biases = first_hidden_layer.weights\n",
    "print(weights)\n",
    "print('\\n')\n",
    "print(biases)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'dense_8/kernel:0' shape=(784, 500) dtype=float32, numpy=\n",
      "array([[-0.00813604,  0.03168706, -0.05305059, ...,  0.02089126,\n",
      "         0.05762189,  0.00806102],\n",
      "       [-0.03202255, -0.00113513, -0.00063752, ...,  0.02969674,\n",
      "        -0.02906536,  0.06214175],\n",
      "       [-0.00365933, -0.0528394 ,  0.00778467, ..., -0.04077482,\n",
      "        -0.02734664, -0.04193263],\n",
      "       ...,\n",
      "       [-0.06485802, -0.03147631,  0.06642643, ...,  0.04519431,\n",
      "         0.01096153,  0.06427322],\n",
      "       [ 0.01740086,  0.06082692,  0.0420944 , ...,  0.00896262,\n",
      "        -0.04954597,  0.00289805],\n",
      "       [ 0.00587015, -0.01560085,  0.02254087, ..., -0.0214595 ,\n",
      "         0.02695014,  0.06172606]], dtype=float32)>\n",
      "\n",
      "\n",
      "<tf.Variable 'dense_8/bias:0' shape=(500,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "layer = model.get_layer(index=1)\n",
    "print(layer.weights)\n",
    "print('\\n')\n",
    "print(layer.bias)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<tf.Variable 'dense_8/kernel:0' shape=(784, 500) dtype=float32, numpy=\n",
      "array([[-0.00813604,  0.03168706, -0.05305059, ...,  0.02089126,\n",
      "         0.05762189,  0.00806102],\n",
      "       [-0.03202255, -0.00113513, -0.00063752, ...,  0.02969674,\n",
      "        -0.02906536,  0.06214175],\n",
      "       [-0.00365933, -0.0528394 ,  0.00778467, ..., -0.04077482,\n",
      "        -0.02734664, -0.04193263],\n",
      "       ...,\n",
      "       [-0.06485802, -0.03147631,  0.06642643, ...,  0.04519431,\n",
      "         0.01096153,  0.06427322],\n",
      "       [ 0.01740086,  0.06082692,  0.0420944 , ...,  0.00896262,\n",
      "        -0.04954597,  0.00289805],\n",
      "       [ 0.00587015, -0.01560085,  0.02254087, ..., -0.0214595 ,\n",
      "         0.02695014,  0.06172606]], dtype=float32)>, <tf.Variable 'dense_8/bias:0' shape=(500,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n",
      "\n",
      "\n",
      "<tf.Variable 'dense_8/bias:0' shape=(500,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('deepcourse': conda)"
  },
  "interpreter": {
   "hash": "dc00d3c18918e3d7b202c635280a2defd5a3767f6fcbbc54f3a1ab7d5490b6ae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}