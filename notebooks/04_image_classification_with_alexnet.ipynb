{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Classification With A Deep Convolutional Neural Network (AlexNet)\n",
    "This notebook details the steps taken to implement and train a Deep Neural Network(DNN) on the [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset in order to perform the computer vision task of image classification on the Cifar-10 test dataset partition. \n",
    "\n",
    "To build, train and test a neural network, the libraries TensorFlow and Keras are leveraged to provide the features and methods required to build the components of a neural network. Supporting libraries such as Matplotlib and Numpy are utlised for visualisation and data processing\n",
    "\n",
    "**[TensorFlow](https://www.tensorflow.org/)**: An open-source platform for implementing, training, and deploying machine learning models.\n",
    "\n",
    "**[Keras](https://keras.io/)**: An open-source library used to implement neural network architectures that run on both CPUs and GPUs.\n",
    "\n",
    "**[Matplotlib](https://matplotlib.org/)**: Tool utilized to create visualization plots in Python such as charts, graphs and more"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cifar-10 Dataset\n",
    "\n",
    "The [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of images of animals and everyday vehicles.\n",
    "The dataset was put together by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. More specifically, it contains 50,000 training examples and 10,000 testing examples, that are all color images with the dimension 32 x 32 categorized into 10 classes.\n",
    "\n",
    "[**Keras Dataset**](https://keras.io/api/datasets/) (keras.datasets)\n",
    "\n",
    "In the practical world of machine learning, obtaining dataset is costly in terms of time, money and effort.\n",
    "But a few introductory datasets have been collated together to help students and researchers build and analyse the performance of models on specifics datasets.\n",
    "Some of these dataset reside within the Keras API and are easily accessible through the [load_data()](https://keras.io/api/datasets/cifar10/#loaddata-function) method of each respective dataset.\n",
    "\n",
    "To load the Cifar-10 dataset: `keras.datasets.cifar10.load_data()`\n",
    "\n",
    "**Classes**\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | airplane    |\n",
    "| 1     | automobile  |\n",
    "| 2     | bird        |\n",
    "| 3     | cat         |\n",
    "| 4     | deer        |\n",
    "| 5     | dog         |\n",
    "| 6     | frog        |\n",
    "| 7     | horse       |\n",
    "| 8     | ship        |\n",
    "| 9     | truck       |\n",
    "\n",
    "**Dataset Partitions**\n",
    "\n",
    "For this particular classification task, 45,000 training images, 10,000 test images, and 5,000 validation images are utilized.\n",
    "- Training Data: This is the group of our dataset used to train the neural network directly. Training data refers to the dataset partition exposed to the neural network during training.\n",
    "- Validation Data: This group of the dataset is utilized during training to assess the performance of the network at various iterations.\n",
    "- Test Data: This partition of the dataset evaluates the performance of our network after the completion of the training phase.\n",
    "\n",
    "**The code snippet below unpacks the tuple of numpy arrays that correspons to the training images and labels, as well as the testing images and labels**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation Data\n",
    "The validation partition of the dataset is obtained from taking a slice of the training data. More specifically, the first 5000 training images and labels are assigned to the validation data.\n",
    "The new training data is reassigned to every training data element but the first 5000 images in the original training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000]\r\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input Pipeline and TensorFlow Dataset\n",
    "\n",
    "Input pipelines are used as a method of streaming data to a model in an automated fashion. Input pipelines can be made responsible for the automation of preprocessing steps taken on input data before presented to the network during training.\n",
    "\n",
    "In this notebook we create input pipelines for our training, testing and validation data using the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API. With well constructed pipelines transformations and augmentation can be made to data points or batches of the dataset partitions in an memory efficient manner.\n",
    "\n",
    "The source of the data used to create the Dataset Object for our input pipeline is the numpy arrays of the dataset partitions created in the cells above. More specfically the `tf.data.Dataset.from_tensor_slices()` allows for the creation of the required Dataset object.\n",
    "\n",
    "Below we create the Dataset Objects `train_da`, `test_ds` and `validation_ds` which represent the input pipelines for the training, testing and validation data respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\r\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualising Training Data\n",
    "\n",
    "The Dataset object returned tf.data.Dataset can be iterated over, so we can view a portion of the dataset using pyplot.\n",
    "\n",
    "Below is a visualisation of the first five images and corresponding labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "plt.figure(figsize=(20,20))\r\n",
    "for i, (image, label) in enumerate(train_ds.take(5)):\r\n",
    "    ax = plt.subplot(5,5,i+1)\r\n",
    "    plt.imshow(image)\r\n",
    "    plt.title(CLASS_NAMES[label.numpy()[0]])\r\n",
    "    plt.axis('off')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Preproccessing\n",
    "\n",
    "**Standardization**\n",
    "The TensorFlow module [tf.image](https://www.tensorflow.org/api_docs/python/tf/image) provides a suite of processing and transformation functions that can be applied on image dataset, so resizing, recoloring, cropping and other common transformation become trivial. \n",
    "\n",
    "In this notebook the images are resized from the original dimensions of 32x32 to 227x227, this is because the input layer of the AlexNet architecture expects images of size 227x227. In addition to resizing, the images are normalized to have a mean of 0 and standard deviation of 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def image_preprocessing(image, label):\r\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\r\n",
    "    image = tf.image.per_image_standardization(image)\r\n",
    "    # Resize images from 32x32 to 277x277\r\n",
    "    image = tf.image.resize(image, (227,227))\r\n",
    "    return image, label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finializing the Input pipelines\n",
    "\n",
    "\n",
    "The input pipelines required for this notebook will automate the following\n",
    "- **Transformation** based on custom function `image_preprocessing` passed into [Dataset.map()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method.\n",
    "- **Caching** the dataset after the first pass through all data points, this is done using the [Dataset.cache()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) method.\n",
    "- **Shuffling** of the dataset to ensure the elements are randomized, this is done using the [Dataset.shuffle()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle). The `buffer_size` argument specifies the number of element passed into the buffer from which random elements are selected.\n",
    "- **Batching** of the dataset in order to present input data in samples set. The batch size is specified by the `batch_size` argument of the [Dataset.batch()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) method ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\r\n",
    "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\r\n",
    "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\r\n",
    "print(\"Training data size:\", train_ds_size)\r\n",
    "print(\"Test data size:\", test_ds_size)\r\n",
    "print(\"Validation data size:\", validation_ds_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data size: 45000\n",
      "Test data size: 10000\n",
      "Validation data size: 5000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_ds = (train_ds\r\n",
    "                  .map(image_preprocessing)\r\n",
    "                  .cache()\r\n",
    "                  .shuffle(buffer_size=train_ds_size, reshuffle_each_iteration=True)\r\n",
    "                  .batch(batch_size=32, drop_remainder=True))\r\n",
    "\r\n",
    "test_ds = (test_ds\r\n",
    "                  .map(image_preprocessing)\r\n",
    "                  .cache()\r\n",
    "                  .shuffle(buffer_size=train_ds_size, reshuffle_each_iteration=True)\r\n",
    "                  .batch(batch_size=32, drop_remainder=True))\r\n",
    "\r\n",
    "validation_ds = (validation_ds\r\n",
    "                  .map(image_preprocessing)\r\n",
    "                  .cache()\r\n",
    "                  .shuffle(buffer_size=train_ds_size, reshuffle_each_iteration=True)\r\n",
    "                  .batch(batch_size=32, drop_remainder=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network Implementation(CNN) Components\n",
    "\n",
    "1. [ Keras Layers API](https://keras.io/api/layers) (keras.layers)\n",
    "    \n",
    "    Layers within Keras allow for the composition of neural networks as they are the fundamental components of neural networks in Keras.\n",
    "    Layers within Keras also house the following: weights of the neural network and functions that act upon inputs and provide outputs, typically to the next layer.\n",
    "    \n",
    "2. [Conv2D Layer](https://keras.io/api/layers/convolution_layers/convolution2d/) (keras.layers.Conv2D)\n",
    "\n",
    "    We are using the tf.keras.layers.Conv2D class to construct the convolutional layers within the convolutional neural network.\n",
    "    In the convolutional layer, a convolution operation takes place between the input image and the kernel/filter of the conv layer. The output of the convolutional layer is the result of the convolution operation between the input data and the values of the kernel/filter.\n",
    "\n",
    "    The Conv2D class constructor `keras.layers.Conv2D` takes several arguments. Below are the ones used within this notebook\n",
    "    - `filters`: The kernel/filter is the name given to the window containing the weight values utilized during the convolution of the weight values with the input values. \n",
    "    - `kernel_size`: Indicative of each unit or neuron's local receptive field size within a convolutional layer. \n",
    "    - `strides`: Defines the amount of shift the filter/sliding window takes over the input image.\n",
    "    - `activation`: A mathematical operation that transforms the result or signals of neurons into a normalized output.\n",
    "    - `input_shape`: The expected shape of the input data to be fed forward through the neural network.\n",
    "    - `padding`: An assigned value of 'same' results in the output shape of the layer been the same as the input, as the output data is padded with zeros around the edges. An assigned value of 'valid' will indicate no padding should be assigned to the output, resulting in the filter/kernel of the convolutional layer only operating parts of the input image that covers the filter/kernel size.\n",
    "\n",
    "\n",
    "3. [BatchNormalization Layer](https://keras.io/api/layers/normalization_layers/batch_normalization/) (keras.layers.BatchNormalization)\n",
    "    \n",
    "    Batch Normalization(BN) is a technique that mitigates the effect of unstable gradients within deep neural networks. BN introduces an additional layer to the neural network that performs operations on the inputs from the previous layer.\n",
    "    The operation standardizes and normalizes the input values. The input values are then transformed through scaling and shifting operations.\n",
    "    The technique batch normalization was presented in 2015 by Christian Szegedy and Sergey Ioffe in this [paper](https://arxiv.org/pdf/1502.03167.pdf).\n",
    "\n",
    "4. [Max Pooling Layer](https://keras.io/api/layers/pooling_layers/max_pooling2d/) (keras.layers.MaxPool2D)\n",
    "\n",
    "    Max pooling is a type of sub-sampling where the maximum pixel value of a set of pixels that fall within the receptive field of a unit within a sub-sampling layer is taken as the output.\n",
    "    \n",
    "    | 30 | 28  | 28 | 184 |\n",
    "    |----|-----|----|-----|\n",
    "    | 0  | 100 | 12 | 98  |       \n",
    "    | 12 | 11  | 9  | 4   |\n",
    "    | 12 | 1   | 45 | 6   |\n",
    "\n",
    "    The above pixel values transform to the set of pixel values below with the max-pooling operation.\n",
    "    \n",
    "    | 100 | 184 |\n",
    "    |-----|-----|\n",
    "    | 12  | 45  |\n",
    "\n",
    "    The max-pooling layer constructor `keras.layers.MaxPool2D` expects the following arguments\n",
    "    - `pool_size`: Dimensions of the sliding window that performs a mathematical operation within pixel values that falls within it.\n",
    "    - `strides`: Indicates the amount the pooling window moves across the input data after the evaluation of each pooling operation.\n",
    "    \n",
    "5. [Flatten Layer](https://keras.io/api/layers/reshaping_layers/flatten/) (keras.layers.Flatten)\n",
    "\n",
    "    The  Flatten layer is known as one of the reshaping layers Keras provides to modify the dimensionalities of inputs.\n",
    "    The Flatten class acts upon the inputs by reducing the dimensionality of the input data to one.\n",
    "    Image datasets are multidimensional, and for input data to be fed forward through the neural network, the dimensions of the input data need to be reduced to one. We essentially require our input data to be 1-dimensional.\n",
    "    For example, an input to the Flatten layer with the shape (None, 10, 2) will provide the output (None, 20).\n",
    "\n",
    "    The input shape of the first layer of a neural network should match the shape of the input data; hence the 'input_shape' attribute of the Flatten layer is (28,28) when using the FashionMNIST dataset (shown in the notebook 02_image_classification_with_DNN).\n",
    "\n",
    "6. [Dense Layer](https://keras.io/api/layers/core_layers/dense/) (keras.layers.Dense)\n",
    "\n",
    "    The dense layer houses neurons within the neural network. The 'unit' attribute specifies the number of neurons within a dense layer. All neurons/units within the dense layer receive input from the previous layer.\n",
    "    The dense layer operation on its input is a matrix-vector multiplication between the input data, learnable weights of the layer, and biases.\n",
    "\n",
    "7. [Activation Functions](https://keras.io/api/layers/activations/) (keras.activations.relu / keras.activations.softmax)\n",
    "\n",
    "    Activation Function: A mathematical operation that transforms the result or signals of neurons into a normalized output. An activation function is a component of a neural network that introduces non-linearity within the network. The inclusion of the activation function enables the neural network to have greater representational power and solve complex functions.\n",
    "\n",
    "    **Examples of Activation functions**\n",
    "\n",
    "    ReLU activation: Stands for rectified linear unit ( y=max(0, x)). It's a type of activation function that transforms the value results of a neuron. The transformation imposed by ReLU on values from a neuron is represented by the formula y=max(0,x). The ReLU activation function clamps down any negative values from the neuron to 0, and positive values remain unchanged. This mathematical transformation is utilized as the output of the current layer and as input to the next.\n",
    "\n",
    "    Softmax: An activation function that derives the probability distribution of a set of numbers within an input vector. The output of a softmax activation function is a vector whose set of values represents the probability of an occurrence of a class/event. The values within the vector all add up to 1.\n",
    "\n",
    "8. [Dropout Layer](https://keras.io/api/layers/regularization_layers/dropout/) (keras.layers.Dropout)\n",
    "\n",
    "    Dropout is a technique that is utilized to reduce a model's potential to overfit.\n",
    "    The dropout technique works by adding a probability factor to the activation of neurons within the layers of a CNN. This probability factor indicates the neurons' chances of being activated during a current feedforward step and during the process of backpropagation.\n",
    "    Dropout is useful as it enables the neurons to reduce dependability on neighbouring neurons; each neuron learns more useful features due to this.\n",
    "\n",
    "**Below is a deep convolutional neural network implementation using all the layers and components described above**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = keras.models.Sequential([\r\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
    "    keras.layers.Flatten(),\r\n",
    "    keras.layers.Dense(4096, activation='relu'),\r\n",
    "    keras.layers.Dropout(0.5),\r\n",
    "    keras.layers.Dense(4096, activation='relu'),\r\n",
    "    keras.layers.Dropout(0.5),\r\n",
    "    keras.layers.Dense(10, activation='softmax')\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TensorBoard\n",
    "\n",
    "TensorBoard provides visual insights into the events that occur during training at each epoch.\n",
    "The training visualization provided by TensorBoard is stored in a runs folder directory. We create a function to generate a folder directory and identify each log via a timestamp.\n",
    "\n",
    "**Run Tensorboard using the command below from the location of the run folder on the terminal**\n",
    "`tensorboard --logdir='runs'`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Configure TensorBoard storage location\r\n",
    "root_logdir = os.path.join(os.curdir, \"runs\")\r\n",
    "def get_run_logdir():\r\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\r\n",
    "    return os.path.join(root_logdir, run_id)\r\n",
    "run_logdir = get_run_logdir()\r\n",
    "\r\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Hyperparameters and More\n",
    "### Compilation\n",
    "Keras provides the `Model.compile()` method which is available to the `model` object instantiated earlier in this notebook. The compile function enables the actual building of the model we have implemented behind the scene with some additional characteristics such as the `loss` function, `optimizer`, and metrics.\n",
    "\n",
    "To train the network, we utilize a loss function that calculates the difference between the predicted values provided by the network and actual values of the training data.\n",
    "\n",
    "The `learning rate` loss values and the optimizer algorithm facilitates the number of changes made to the weights within the network. \n",
    "\n",
    "Supporting factors such as `momentum` and `learning rate schedule`, provide the ideal environment to enable the network training to converge, herby getting the training loss values as close to zero as possible.\n",
    "\n",
    "### Definitions\n",
    "* **Learning Rate**: An integral component of a neural network as its a factor value that determines the level of updates that are made to the values of the weights of the network.\n",
    "\n",
    "    In a visualization exercise, the function to be solved can be depicted as a hyperbolic curve in n-dimensional parameter space.\n",
    "    The learning rate is a component that affects the step size that the current parameter values take towards a local/global minima; hence the learning rate directly affects the rate of convergence of a network during training. \n",
    "\n",
    "    If the learning rate is too small the network might take several iterations and epochs to converge. On the other hand, if the learning rate is too high, there is a risk of overshooting the minima, and as a result of this our training doesnt converge. Selecting the appropriate learning rate can be a time staking exercise.\n",
    "    \n",
    "* **Learning rate schedule**: A constant learning rate can be utilized during the training of a neural network, but this can increase the amount of training that has to take place to arrive at optimal neural network performance. By utilizing the learning rate schedule, we introduce a timely reduction or increment of the learning rate during training to arrive at an optimal training outcome of the neural network.\n",
    "\n",
    "* **Learning rate Decay**: Learning rate decay reduces the oscillations of steps taken towards a local minimum during gradient descent. By reducing the learning rate to a smaller value compares to the learning rate value utilized at the start of the training, we can steer the network towards a solution that oscillates in smaller ranges around a minimum.\n",
    "\n",
    "* **Loss Function**: This is a method that quantifies how well a machine learning model performs. The quantification is an output(cost) based on a set of inputs, which are referred to as parameter values. The parameter values are used to estimate a prediction, and the loss is the difference between the prediction and the actual values.\n",
    "\n",
    "* **Optimizer**: An optimizer within a neural network is an algorithmic implementation that facilitates the process of gradient descent within a neural network by minimizing the loss values provided via the loss function.\n",
    "\n",
    "\n",
    "### More Explanations\n",
    "In the following cell the optimizer used for model compilation if instansiated along with sets of values passed into the Keras's implementation of Stotastic Gradient Decent (SGD) [`keras.optimizers.SGD()`](https://keras.io/api/optimizers/sgd/)\n",
    "\n",
    "Let's paint a picture. We have a loss function, and we need to find the optimum solution to solve the loss function. Here comes gradient descent, an algorithm that works by making changes to the values of the parameters/weight values that are within the model, all in the purpose of minimizing the cost function. An example of a loss function is Mean Squared Error.\n",
    "\n",
    "Gradient descent intrinsic functionality works by finding the direction to take towards a local minimum based on the calculated gradient obtained from the error function with respect to the parameters at a particular data point.\n",
    "\n",
    "Stotastic Gradient Decent (SGD) works by actually picking a single data point from the training set and computing the gradient, based on this single data point. Updates made within the parameter space during gradient descent can be noisy when using SGD. The noisiness characteristic of SGD is a result of its random nature that occurs when selecting data points from the training set to compute gradients from at each step.\n",
    "\n",
    "The loss function leveraged for the neural network implemented in this notebook is [Sparse Categorical Crossentropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class). This loss function is preferred when conducting classification on multiple labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'])\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 55, 55, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 256)       98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 56,361,738\n",
      "Trainable params: 56,358,986\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Convolutional Neural Network\n",
    "\n",
    "The Models API [fit](https://keras.io/api/models/model_training_apis/#fit-method) method provides the tools to train the implemented network.\n",
    "\n",
    "Passing specific arguments into the fit function enables the following:\n",
    "* Specify the training data used for training `x` takes the training images and `y` accepts the numpy array  of training labels\n",
    "* The `epochs` argument accepts an integer that corresponds to the numner of epochs the neural network is to be trained for\n",
    "* Validation dataset to be used to validate the performance of the network during training to unseen data, specified by the `validation_data` argument\n",
    "* We also utilize the `callbacks` argument, which in this instance, takes a list of the tensorboard callback created earlier and the early stopping callback created in the cell below.\n",
    "* `batch_size`: Dicates the amound of data points presented to the network before a gradient update. If a batch size numner is not specified, the default batch size within Keras when training a neural network is 32. The network implemented in this notebook is trained for a total of 60 epochs. \n",
    "    \n",
    "\n",
    "\n",
    "**In summary, we train the model for a maximum of 30 epochs, where we feed forward all our training data in batches of 32 (batch size) through the network at each epoch.**\n",
    "**An update is made to our networks weights parameters after its seen 32 training images and labels.**\n",
    "**The fit method takes additional arguments that are in the [official Keras documentation](https://keras.io/api/models/model_training_apis/#fit-method).**\n",
    "**With the utilization of early stopping, a halt to training is made once no improvement in the validation loss is recorded after 3 epochs. Early stopping can save you hours, especially in the scenario where your network begins to overfit and stops converging.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model.fit(train_ds,\r\n",
    "          epochs=30,\r\n",
    "          validation_data=validation_ds,\r\n",
    "          validation_freq=1,\r\n",
    "          callbacks=[tensorboard_cb])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "   1/1406 [..............................] - ETA: 0s - loss: 5.0672 - accuracy: 0.0938WARNING:tensorflow:From C:\\Users\\LoveShark\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1406 [..............................] - ETA: 40s - loss: 4.7916 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0455s). Check your callbacks.\n",
      "1406/1406 [==============================] - 38s 27ms/step - loss: 2.1344 - accuracy: 0.3166 - val_loss: 1.4792 - val_accuracy: 0.4782\n",
      "Epoch 2/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 1.6057 - accuracy: 0.4282 - val_loss: 1.3987 - val_accuracy: 0.5094\n",
      "Epoch 3/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.4532 - accuracy: 0.4812 - val_loss: 1.2462 - val_accuracy: 0.5681\n",
      "Epoch 4/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.3412 - accuracy: 0.5262 - val_loss: 1.1854 - val_accuracy: 0.5911\n",
      "Epoch 5/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.2499 - accuracy: 0.5560 - val_loss: 1.1151 - val_accuracy: 0.6138\n",
      "Epoch 6/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 1.1830 - accuracy: 0.5824 - val_loss: 1.0743 - val_accuracy: 0.6362\n",
      "Epoch 7/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 1.1181 - accuracy: 0.6049 - val_loss: 1.0232 - val_accuracy: 0.6506\n",
      "Epoch 8/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 1.0657 - accuracy: 0.6250 - val_loss: 0.9994 - val_accuracy: 0.6532\n",
      "Epoch 9/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 1.0215 - accuracy: 0.6406 - val_loss: 0.9672 - val_accuracy: 0.6735\n",
      "Epoch 10/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.9749 - accuracy: 0.6567 - val_loss: 0.9420 - val_accuracy: 0.6761\n",
      "Epoch 11/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.9329 - accuracy: 0.6710 - val_loss: 0.9344 - val_accuracy: 0.6801\n",
      "Epoch 12/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.8909 - accuracy: 0.6869 - val_loss: 0.9100 - val_accuracy: 0.6821\n",
      "Epoch 13/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.8599 - accuracy: 0.6980 - val_loss: 0.8862 - val_accuracy: 0.6975\n",
      "Epoch 14/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.8290 - accuracy: 0.7086 - val_loss: 0.8680 - val_accuracy: 0.6983\n",
      "Epoch 15/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7903 - accuracy: 0.7214 - val_loss: 0.8597 - val_accuracy: 0.7017\n",
      "Epoch 16/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7578 - accuracy: 0.7341 - val_loss: 0.8757 - val_accuracy: 0.7001\n",
      "Epoch 17/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7290 - accuracy: 0.7441 - val_loss: 0.8401 - val_accuracy: 0.7167\n",
      "Epoch 18/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.6954 - accuracy: 0.7556 - val_loss: 0.8488 - val_accuracy: 0.7119\n",
      "Epoch 19/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.6742 - accuracy: 0.7639 - val_loss: 0.8293 - val_accuracy: 0.7163\n",
      "Epoch 20/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.6395 - accuracy: 0.7763 - val_loss: 0.8159 - val_accuracy: 0.7190\n",
      "Epoch 21/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.6135 - accuracy: 0.7846 - val_loss: 0.8094 - val_accuracy: 0.7254\n",
      "Epoch 22/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.5849 - accuracy: 0.7959 - val_loss: 0.8123 - val_accuracy: 0.7244\n",
      "Epoch 23/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.5647 - accuracy: 0.8003 - val_loss: 0.8187 - val_accuracy: 0.7236\n",
      "Epoch 24/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.5315 - accuracy: 0.8124 - val_loss: 0.8084 - val_accuracy: 0.7290\n",
      "Epoch 25/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.5109 - accuracy: 0.8211 - val_loss: 0.8175 - val_accuracy: 0.7320\n",
      "Epoch 26/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.4842 - accuracy: 0.8319 - val_loss: 0.8229 - val_accuracy: 0.7270\n",
      "Epoch 27/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.4614 - accuracy: 0.8399 - val_loss: 0.8408 - val_accuracy: 0.7232\n",
      "Epoch 28/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.4349 - accuracy: 0.8478 - val_loss: 0.8305 - val_accuracy: 0.7260\n",
      "Epoch 29/30\n",
      "1406/1406 [==============================] - 35s 25ms/step - loss: 0.4193 - accuracy: 0.8537 - val_loss: 0.8253 - val_accuracy: 0.7322\n",
      "Epoch 30/30\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.3930 - accuracy: 0.8644 - val_loss: 0.8227 - val_accuracy: 0.7362\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x176977d55c8>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evalating the Model\n",
    "\n",
    "To evaluate a model requires feed-forwarding through the network data that hasnt been exposed to the network during training.\n",
    "Evaluation of the model before actual utilization is a good indicator of observing how well the model can generalize to unseen data and perform in the 'wild'.\n",
    "With the evaluation results, you can decide either to fine-tune the network hyperparameters or move forward to production after observing the accuracy of the evaluation over the test dataset.\n",
    "\n",
    "Evaluation with the Keras library is easy as we simply call the [Model.evaluate()](https://keras.io/api/models/model_training_apis/#evaluate-method) method and pass the test data to the model.\n",
    "\n",
    "In the next cell we evaluate the trained model on the test data. Test data is presented to the model in batches of 32 (default `batch_size` number if not specified in the arguments)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model.evaluate(test_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "312/312 [==============================] - 4s 14ms/step - loss: 0.8737 - accuracy: 0.7199\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.8736509680747986, 0.7198517918586731]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# AlexNet with Augmentation layers\r\n",
    "model = keras.models.Sequential([\r\n",
    "    keras.layers.experimental.preprocessing.RandomRotation(0.2, input_shape=(227,227,3)),\r\n",
    "    keras.layers.experimental.preprocessing.RandomZoom(0.1),\r\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\r\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu'),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
    "    keras.layers.BatchNormalization(),\r\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
    "    keras.layers.Flatten(),\r\n",
    "    keras.layers.Dense(4096, activation='relu'),\r\n",
    "    keras.layers.Dropout(0.5),\r\n",
    "    keras.layers.Dense(4096, activation='relu'),\r\n",
    "    keras.layers.Dropout(0.5),\r\n",
    "    keras.layers.Dense(10, activation='softmax')\r\n",
    "])\r\n",
    "\r\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\r\n",
    "\r\n",
    "model.fit(train_ds,\r\n",
    "          epochs=30,\r\n",
    "          validation_data=validation_ds,\r\n",
    "          validation_freq=1,\r\n",
    "          callbacks=[tensorboard_cb])\r\n",
    "\r\n",
    "model.evaluate(test_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/400\n",
      "   2/1406 [..............................] - ETA: 9:26 - loss: 9.8702 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.7865s). Check your callbacks.\n",
      "1406/1406 [==============================] - 37s 26ms/step - loss: 2.3997 - accuracy: 0.2552 - val_loss: 1.8162 - val_accuracy: 0.3620\n",
      "Epoch 2/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.8482 - accuracy: 0.3443 - val_loss: 1.6945 - val_accuracy: 0.4209\n",
      "Epoch 3/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.6851 - accuracy: 0.4001 - val_loss: 1.5356 - val_accuracy: 0.4752\n",
      "Epoch 4/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.5642 - accuracy: 0.4413 - val_loss: 1.3891 - val_accuracy: 0.5182\n",
      "Epoch 5/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.4824 - accuracy: 0.4729 - val_loss: 1.3782 - val_accuracy: 0.5276\n",
      "Epoch 6/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.4175 - accuracy: 0.4990 - val_loss: 1.2760 - val_accuracy: 0.5641\n",
      "Epoch 7/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.3627 - accuracy: 0.5174 - val_loss: 1.2122 - val_accuracy: 0.5869\n",
      "Epoch 8/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.3249 - accuracy: 0.5339 - val_loss: 1.2065 - val_accuracy: 0.5980\n",
      "Epoch 9/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.2834 - accuracy: 0.5477 - val_loss: 1.1167 - val_accuracy: 0.6156\n",
      "Epoch 10/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.2500 - accuracy: 0.5609 - val_loss: 1.1359 - val_accuracy: 0.6156\n",
      "Epoch 11/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.2168 - accuracy: 0.5697 - val_loss: 1.0946 - val_accuracy: 0.6382\n",
      "Epoch 12/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.1914 - accuracy: 0.5809 - val_loss: 1.0999 - val_accuracy: 0.6362\n",
      "Epoch 13/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.1693 - accuracy: 0.5947 - val_loss: 1.0726 - val_accuracy: 0.6390\n",
      "Epoch 14/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.1416 - accuracy: 0.6008 - val_loss: 1.1092 - val_accuracy: 0.6366\n",
      "Epoch 15/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.1219 - accuracy: 0.6090 - val_loss: 1.0472 - val_accuracy: 0.6412\n",
      "Epoch 16/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.1074 - accuracy: 0.6106 - val_loss: 1.0060 - val_accuracy: 0.6623\n",
      "Epoch 17/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.0812 - accuracy: 0.6247 - val_loss: 0.9845 - val_accuracy: 0.6671\n",
      "Epoch 18/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.0677 - accuracy: 0.6284 - val_loss: 1.0273 - val_accuracy: 0.6637\n",
      "Epoch 19/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.0468 - accuracy: 0.6363 - val_loss: 0.9799 - val_accuracy: 0.6795\n",
      "Epoch 20/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.0362 - accuracy: 0.6421 - val_loss: 0.9603 - val_accuracy: 0.6815\n",
      "Epoch 21/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.0152 - accuracy: 0.6466 - val_loss: 0.9983 - val_accuracy: 0.6737\n",
      "Epoch 22/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 1.0055 - accuracy: 0.6520 - val_loss: 0.9740 - val_accuracy: 0.6851\n",
      "Epoch 23/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9928 - accuracy: 0.6544 - val_loss: 1.0046 - val_accuracy: 0.6639\n",
      "Epoch 24/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9822 - accuracy: 0.6605 - val_loss: 0.9434 - val_accuracy: 0.7029\n",
      "Epoch 25/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9676 - accuracy: 0.6645 - val_loss: 0.9188 - val_accuracy: 0.7031\n",
      "Epoch 26/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9584 - accuracy: 0.6687 - val_loss: 0.9350 - val_accuracy: 0.6919\n",
      "Epoch 27/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9492 - accuracy: 0.6715 - val_loss: 0.9012 - val_accuracy: 0.7097\n",
      "Epoch 28/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9276 - accuracy: 0.6760 - val_loss: 0.8945 - val_accuracy: 0.7145\n",
      "Epoch 29/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9239 - accuracy: 0.6785 - val_loss: 0.9358 - val_accuracy: 0.7019\n",
      "Epoch 30/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9087 - accuracy: 0.6843 - val_loss: 0.8836 - val_accuracy: 0.7095\n",
      "Epoch 31/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.9047 - accuracy: 0.6863 - val_loss: 0.8784 - val_accuracy: 0.7202\n",
      "Epoch 32/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8905 - accuracy: 0.6897 - val_loss: 0.8952 - val_accuracy: 0.7175\n",
      "Epoch 33/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8836 - accuracy: 0.6943 - val_loss: 0.8681 - val_accuracy: 0.7173\n",
      "Epoch 34/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8711 - accuracy: 0.6995 - val_loss: 0.8445 - val_accuracy: 0.7282\n",
      "Epoch 35/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8664 - accuracy: 0.7021 - val_loss: 0.8730 - val_accuracy: 0.7216\n",
      "Epoch 36/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8511 - accuracy: 0.7072 - val_loss: 0.8446 - val_accuracy: 0.7270\n",
      "Epoch 37/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8511 - accuracy: 0.7062 - val_loss: 0.8799 - val_accuracy: 0.7240\n",
      "Epoch 38/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8386 - accuracy: 0.7107 - val_loss: 0.8794 - val_accuracy: 0.7268\n",
      "Epoch 39/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8325 - accuracy: 0.7103 - val_loss: 0.8502 - val_accuracy: 0.7294\n",
      "Epoch 40/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8280 - accuracy: 0.7146 - val_loss: 0.8564 - val_accuracy: 0.7304\n",
      "Epoch 41/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8150 - accuracy: 0.7192 - val_loss: 0.8507 - val_accuracy: 0.7348\n",
      "Epoch 42/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8088 - accuracy: 0.7216 - val_loss: 0.8607 - val_accuracy: 0.7131\n",
      "Epoch 43/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.8014 - accuracy: 0.7241 - val_loss: 0.8300 - val_accuracy: 0.7274\n",
      "Epoch 44/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7961 - accuracy: 0.7256 - val_loss: 0.8226 - val_accuracy: 0.7388\n",
      "Epoch 45/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7936 - accuracy: 0.7251 - val_loss: 0.8717 - val_accuracy: 0.7306\n",
      "Epoch 46/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7836 - accuracy: 0.7308 - val_loss: 0.8594 - val_accuracy: 0.7194\n",
      "Epoch 47/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7701 - accuracy: 0.7346 - val_loss: 0.8594 - val_accuracy: 0.7278\n",
      "Epoch 48/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7730 - accuracy: 0.7338 - val_loss: 0.7886 - val_accuracy: 0.7438\n",
      "Epoch 49/400\n",
      "1406/1406 [==============================] - 36s 25ms/step - loss: 0.7717 - accuracy: 0.7341 - val_loss: 0.8334 - val_accuracy: 0.7330\n",
      "Epoch 50/400\n",
      "1406/1406 [==============================] - 37s 27ms/step - loss: 0.7647 - accuracy: 0.7374 - val_loss: 0.8304 - val_accuracy: 0.7300\n",
      "Epoch 51/400\n",
      "1406/1406 [==============================] - 37s 26ms/step - loss: 0.7558 - accuracy: 0.7405 - val_loss: 0.8355 - val_accuracy: 0.7388\n",
      "Epoch 52/400\n",
      "1406/1406 [==============================] - 38s 27ms/step - loss: 0.7502 - accuracy: 0.7402 - val_loss: 0.7981 - val_accuracy: 0.7428\n",
      "Epoch 53/400\n",
      " 486/1406 [=========>....................] - ETA: 23s - loss: 0.7267 - accuracy: 0.7516"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LOVESH~1\\AppData\\Local\\Temp/ipykernel_14480/3033593435.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m           callbacks=[tensorboard_cb])\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('course': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "dfa55811a9239ca509c77afce6366afd1dbbda529588920a51068939e03c093e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}